{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Code of Conduct for Jupyter Notebook Template Usage\n",
                "\n",
                "## Introduction\n",
                "\n",
                "Welcome to our Jupyter Notebook environment. To ensure a productive and respectful environment, we have established a few ground rules. Please adhere to this Code of Conduct when using our Jupyter Notebook templates.\n",
                "\n",
                "## Guidelines\n",
                "\n",
                "### 1. **Notebook Structure**\n",
                "   - This notebook has a specifc structure that should be respected and not to be tempered with.\n",
                "\n",
                "### 2. **Responsible Resource Usage**\n",
                "   - Use computational resources judiciously.\n",
                "   - Avoid unnecessary computational tasks that can overload the system.\n",
                "\n",
                "## Reporting Issues\n",
                "\n",
                "If you encounter any issues or observe violations of this Code of Conduct, please report them to [jarcau.stefan.cristian@gmail.com](jarcau.stefan.cristian@gmail.com).\n",
                "\n",
                "## Conclusion\n",
                "\n",
                "By adhering to these guidelines, we can maintain a healthy, productive, and welcoming environment for all users. Thank you for your cooperation and happy coding!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Getting the dataset from the database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "v35iuYktjj9i"
            },
            "outputs": [],
            "source": [
                "import io\n",
                "import os\n",
                "import requests\n",
                "import pandas as pd\n",
                "\n",
                "url = os.getenv(\"DATASET_URL\")\n",
                "api = os.getenv(\"API\")\n",
                "\n",
                "response = requests.get(api + f\"?path={url}\")\n",
                "\n",
                "if response.status_code != 200:\n",
                "    raise Exception(response.content.decode(\"utf-8\"))\n",
                "\n",
                "df = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "id": "pppcog65Rcei"
            },
            "source": [
                "# Encode literal columns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "id": "FkJG97J1nWrX"
            },
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "encoded_df = df\n",
                "\n",
                "le = LabelEncoder()\n",
                "\n",
                "for column in encoded_df.columns:\n",
                "  if isinstance(encoded_df[column][0], str):\n",
                "    encoded_df[column] = le.fit_transform(encoded_df[column])\n",
                "\n",
                "encoded_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "nIXaMas_OgvC"
            },
            "source": [
                "# Training and choosing the best model (You can modify the parameters of the model to fits the dataset best, or let them as they are)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "id": "6JwiIe6KqeXK"
            },
            "outputs": [],
            "source": [
                "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
                "from metrics import clustering_dispersion_indicator, mean_index_adequacy, variance_ratio_criterion\n",
                "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, MeanShift, AffinityPropagation\n",
                "from tqdm import tqdm\n",
                "from copy import deepcopy\n",
                "\n",
                "# Define the models you want to compare (You can let the default, but you can change the parameters)\n",
                "num_clusters = [i for i in range(16) if i > 3]\n",
                "models = {\n",
                "    'KMeans': KMeans(n_clusters=5, random_state=42),\n",
                "    'Agglomerative Clustering': AgglomerativeClustering(n_clusters=5),\n",
                "    'DBSCAN': DBSCAN(eps= 0.5, min_samples=5),\n",
                "    'Spectral Clustering': SpectralClustering(n_clusters=5, random_state=42),\n",
                "    'Mean Shift': MeanShift(),\n",
                "    \"Affinity Propagation\": AffinityPropagation(damping=0.5, random_state=42),\n",
                "}\n",
                "\n",
                "\n",
                "cluster_results = {}\n",
                "for num_cluster in num_clusters:\n",
                "    print(f\"Caluculating scores for {num_cluster} of custers.\")\n",
                "    models_deep_copy = deepcopy(models)\n",
                "    results = []\n",
                "    for model_name, model in models_deep_copy.items():\n",
                "      if model.get_params().get(\"n_clusters\") is not None:\n",
                "          model.n_clusters = num_cluster # Trying the algorithm for different number of clusters.\n",
                "          \n",
                "      model.fit(encoded_df)\n",
                "      labels = model.labels_\n",
                "      results.append((model_name, {\n",
                "          \"silhouette_score\": round(silhouette_score(encoded_df, labels), 2),\n",
                "          \"davies_bouldin_score\": round(davies_bouldin_score(encoded_df, labels), 2),\n",
                "          \"calinski_harabasz_score\": round(calinski_harabasz_score(encoded_df, labels), 2),\n",
                "          \"cdi\": clustering_dispersion_indicator(encoded_df, labels),\n",
                "          \"mia\": mean_index_adequacy(encoded_df, labels),\n",
                "          \"vrc\": variance_ratio_criterion(encoded_df, labels)\n",
                "      }))\n",
                "\n",
                "    print(f\"Getting the best results out of all the models.\")\n",
                "    best_results = {k: (\"\", 0 if k in [\"silhouette_score\", \"calinski_harabasz_score\", \"vrc\"] else 10000000) for k in results[0][1].keys()}\n",
                "    cluster_result = None\n",
                "    for result in results:\n",
                "        for k, v in result[1].items():\n",
                "            if k in [\"silhouette_score\", \"calinski_harabasz_score\"]:\n",
                "                if v > best_results[k][1]:\n",
                "                    best_results[k] = (result[0], v)\n",
                "            elif k in [\"davies_bouldin_score\", \"vrc\", \"cdi\", \"mia\"]:\n",
                "              if v < best_results[k][1]:\n",
                "                  best_results[k] = (result[0], v)\n",
                "  \n",
                "    print(f\"Getting the best model for {num_cluster} of custers.\")\n",
                "    best_models_cluster = {k: 0 for k in models_deep_copy.keys()}\n",
                "    for best_result in best_results.values():\n",
                "        best_models_cluster[best_result[0]] += 1\n",
                "\n",
                "    cluster_results[num_cluster] = sorted(best_models_cluster.items(), key=lambda item: item[1], reverse=True)[0][0]\n",
                "\n",
                "\n",
                "print(\"Getting the best model out of all the clusters.\")\n",
                "best_models = {k: 0 for k in models.keys()}\n",
                "for cluster_result in cluster_results.values():\n",
                "  best_models[cluster_result] += 1\n",
                "\n",
                "sorted_models = sorted(best_models.items(), key=lambda item: item[1], reverse=True)\n",
                "model_name, number_of_clusters = sorted_models[0][0], sorted_models[0][1]\n",
                "\n",
                "model_name, number_of_clusters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "id": "Rn9IWMuvOnr-"
            },
            "source": [
                "# Calculating the scores for the best model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "5mh7JVRMr2Mq"
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
                "from metrics import clustering_dispersion_indicator, mean_index_adequacy, variance_ratio_criterion\n",
                "\n",
                "if (\"model_name\" in locals() or \"model_name\" in globals()) and (\"models\" in locals() or \"models\" in globals()) and (\"number_of_clusters\" in locals() or \"number_of_clusters\" in globals()):\n",
                "  if models[model_name].get_params().get(\"n_clusters\") is not None:\n",
                "    models[model_name].n_clusters = number_of_clusters\n",
                "\n",
                "  models[model_name].fit(encoded_df)\n",
                "\n",
                "  labels = models[model_name].labels_\n",
                "\n",
                "  sil_score = silhouette_score(encoded_df, labels)\n",
                "  chs = calinski_harabasz_score(encoded_df, labels)\n",
                "  dbs = davies_bouldin_score(encoded_df, labels)\n",
                "  cdi = clustering_dispersion_indicator(encoded_df, labels)\n",
                "  mia = mean_index_adequacy(encoded_df, labels)\n",
                "  vrc = variance_ratio_criterion(encoded_df, labels)\n",
                "\n",
                "  metrics = {\n",
                "    \"Silhoutte Score\": round(sil_score, 2),\n",
                "    \"Calinski Harabasz Score\": round(chs, 2),\n",
                "    \"Davies Bouldin Score\": round(dbs, 2),\n",
                "    \"Clustering Dispersion Indicator\": cdi,\n",
                "    \"Mean Index Adequacy\": mia,\n",
                "    \"Variance Ratio Criterion\": vrc,\n",
                "  }\n",
                "\n",
                "  print(metrics)\n",
                "else:\n",
                "  print(\"Please run all the previouse cell before running this one!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "id": "23K9XBBGOuCi"
            },
            "source": [
                "# Saving the parameters of the best model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "MxFx9IoYMoha"
            },
            "outputs": [],
            "source": [
                "if (\"model_name\" in locals() or \"model_name\" in globals()) and (\"models\" in locals() or \"models\" in globals()):\n",
                "  params = { k: v for k, v in models[model_name].get_params().items() if v is not None}\n",
                "  print(params)\n",
                "else:\n",
                "  print(\"Please run all the previouse cell before running this one!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Saving images of the best model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shap\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "from io import BytesIO\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.decomposition import PCA\n",
                "\n",
                "if (\"model_name\" in locals() or \"model_name\" in globals()) and (\"models\" in locals() or \"models\" in globals()) and (\"labels\" in locals() or \"labels\" in globals()):\n",
                "    pca = PCA(n_components=2)\n",
                "    X_r = pca.fit_transform(encoded_df)\n",
                "\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    sns.scatterplot(x=X_r[:, 0], y=X_r[:, 1], hue=labels, palette=\"viridis\")\n",
                "    plt.title(\"Cluster assignments\")\n",
                "\n",
                "    buf_clusters = BytesIO()\n",
                "    plt.savefig(buf_clusters, format='png')\n",
                "    plt.close()\n",
                "\n",
                "    buf_clusters.seek(0)\n",
                "    pair_plot_with_clusters = Image.open(buf_clusters)\n",
                "\n",
                "    corr = df.corr()\n",
                "\n",
                "    plt.figure(figsize=(10, 7))\n",
                "    sns.heatmap(corr, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
                "    plt.title('Correlation Matrix')\n",
                "\n",
                "    buf_corr = BytesIO()\n",
                "    plt.savefig(buf_corr, format='png')\n",
                "    plt.close()\n",
                "\n",
                "    buf_corr.seek(0)\n",
                "    corr_matrix = Image.open(buf_corr)\n",
                "\n",
                "    # Saving SHAP plot for explainability.\n",
                "    explainer = shap.Explainer(models[model_name].predict, encoded_df)\n",
                "\n",
                "    shap_values = explainer(encoded_df)\n",
                "\n",
                "    shap.summary_plot(shap_values, encoded_df, plot_type=\"bar\", show=False)\n",
                "\n",
                "    buf_shap = BytesIO()\n",
                "    plt.savefig(buf_shap, format='png')\n",
                "    buf_shap.seek(0)\n",
                "    plt.close()\n",
                "\n",
                "    shap_image = Image.open(buf_shap)\n",
                "else:\n",
                "    print(\"Please run all the previouse cell before running this one!\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Save the model inside our model repository (Please run this cell only when you are satisfied with the result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "id": "Tvs46pGrO_-R"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import datetime\n",
                "import mlflow\n",
                "import json\n",
                "import pandas as pd\n",
                "from mlflow import MlflowClient, MlflowException\n",
                "from sqlalchemy import create_engine, Column, String, DateTime, Numeric, Integer\n",
                "from sqlalchemy.orm import sessionmaker, declarative_base\n",
                "\n",
                "def is_variable_defined(var_name):\n",
                "    return var_name in locals() or var_name in globals()\n",
                "\n",
                "if is_variable_defined(\"model_name\") and is_variable_defined(\"models\") and is_variable_defined(\"params\") and is_variable_defined(\"metrics\") and is_variable_defined(\"corr_matrix\") and is_variable_defined(\"pair_plot_with_clusters\") and is_variable_defined(\"shap_image\"):\n",
                "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
                "    register_name = f'{os.getenv(\"MODEL_NAME\")}-{os.getenv(\"USER_ID\")}'\n",
                "\n",
                "    client = MlflowClient()\n",
                "\n",
                "    condition = None\n",
                "    try:\n",
                "        condition = client.get_registered_model(register_name)\n",
                "    except MlflowException as e:\n",
                "        print(e)\n",
                "\n",
                "    if condition is None:\n",
                "        with mlflow.start_run(experiment_id=mlflow.get_experiment_by_name(\"default\").experiment_id) as run:\n",
                "          mlflow.sklearn.log_model(models[model_name], \"model\")\n",
                "          mlflow.log_metrics(metrics)\n",
                "          mlflow.log_params(params)\n",
                "          mlflow.log_image(corr_matrix, \"correlation_matrix.png\")\n",
                "          mlflow.log_image(pair_plot_with_clusters, \"pair_plot_with_clusters.png\")\n",
                "          mlflow.log_image(shap_image, \"shap.png\")\n",
                "\n",
                "        run_id = run.info.run_id\n",
                "\n",
                "        src_uri = f\"runs:/{run_id}/model\"\n",
                "\n",
                "        client.create_registered_model(register_name)\n",
                "        mv = client.create_model_version(register_name, src_uri, run_id)\n",
                "        print(f\"Name: {mv.name}\")\n",
                "        print(f\"Version: {mv.version}\")\n",
                "        print(f\"Source: {mv.source}\")\n",
                "\n",
                "        Base = declarative_base()\n",
                "\n",
                "        class MyTable(Base):\n",
                "            __tablename__ = 'models'\n",
                "            model_id = Column(String, primary_key=True)\n",
                "            created_at = Column(DateTime)\n",
                "            user_id = Column(String)\n",
                "            dataset_user = Column(String)\n",
                "            description = Column(String)\n",
                "            score = Column(Numeric)\n",
                "            model_name = Column(String)\n",
                "            score_count = Column(Integer)\n",
                "            notebook_type = Column(String)\n",
                "\n",
                "        csv_data = {\n",
                "            \"column_dtypes\": {},\n",
                "            \"column_ranges\": {},\n",
                "            \"column_categories\": {},\n",
                "            \"column_unique_values\": {}\n",
                "        }\n",
                "\n",
                "        for column in df.columns:\n",
                "            if pd.api.types.is_numeric_dtype(df[column]):\n",
                "                csv_data[\"column_dtypes\"][column] = \"numeric\"\n",
                "                csv_data[\"column_ranges\"][column] = (min(df[column]), max(df[column]))\n",
                "                csv_data[\"column_categories\"][column] = None\n",
                "                csv_data[\"column_unique_values\"][column] = None\n",
                "            else:\n",
                "                if len(df[column].unique()) == len(df):\n",
                "                    csv_data[\"column_dtypes\"][column] = \"unique_identifier\"\n",
                "                    csv_data[\"column_ranges\"][column] = None\n",
                "                    csv_data[\"column_categories\"][column] = None\n",
                "                    csv_data[\"column_unique_values\"][column] = len(df[column].unique())\n",
                "                else:\n",
                "                    csv_data[\"column_dtypes\"][column] = \"categorical\"\n",
                "                    csv_data[\"column_ranges\"][column] = None\n",
                "                    csv_data[\"column_categories\"][column] = df[column].unique()\n",
                "                    csv_data[\"column_unique_values\"][column] = None\n",
                "\n",
                "        engine = create_engine(f'postgresql+psycopg2://{os.getenv(\"POSTGRES_USER\")}:{os.getenv(\"POSTGRES_PASSWORD\")}@{os.getenv(\"POSTGRES_HOST\")}:{os.getenv(\"POSTGRES_PORT\")}/'\n",
                "                                f'{os.getenv(\"POSTGRES_DB\")}')\n",
                "        Base.metadata.create_all(engine)\n",
                "\n",
                "        Session = sessionmaker(bind=engine)\n",
                "        session = Session()\n",
                "\n",
                "        with Session() as session:\n",
                "            new_row = MyTable(model_id=register_name, created_at=datetime.datetime.now(), user_id=os.getenv(\"USER_ID\"),\n",
                "                             description=json.dumps(csv_data), score=0.0, model_name=os.getenv(\"MODEL_NAME\"), score_count=0, dataset_user=os.getenv(\"DATASET_USER\"), notebook_type=\"clustering\")\n",
                "\n",
                "            session.add(new_row)\n",
                "\n",
                "            session.commit()\n",
                "    else:\n",
                "        print(\"You can only add the model once!\")\n",
                "else:\n",
                "    print(\"Please run all the previouse cell before running this one!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "id": "xDWWZBa5U6wv"
            },
            "source": [
                "# Deleting the notebook. (Be careful when running this cell!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "id": "zTkN_mp5U6EY"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import requests\n",
                "from sqlalchemy.orm import sessionmaker, declarative_base\n",
                "from sqlalchemy import create_engine, Column, String, DateTime, Integer\n",
                "\n",
                "def is_variable_defined(var_name):\n",
                "    return var_name in locals() or var_name in globals()\n",
                "\n",
                "if is_variable_defined(\"model_name\") and is_variable_defined(\"models\") and is_variable_defined(\"params\") and is_variable_defined(\"metrics\") and is_variable_defined(\"corr_matrix\") and is_variable_defined(\"pair_plot_with_clusters\") and is_variable_defined(\"shap_image\"):\n",
                "    Base = declarative_base()\n",
                "    \n",
                "    class MyTable(Base):\n",
                "        __tablename__ = 'notebooks'\n",
                "        user_id = Column(String)\n",
                "        created_at = Column(DateTime)\n",
                "        last_accessed = Column(DateTime)\n",
                "        notebook_id = Column(String, primary_key=True)\n",
                "        description = Column(String)\n",
                "        dataset_user = Column(String)\n",
                "        dataset_name = Column(String)\n",
                "        port = Column(Integer)\n",
                "        notebook_type = Column(String)\n",
                "    \n",
                "    \n",
                "    # Connect to the database\n",
                "    engine = create_engine(f'postgresql+psycopg2://{os.getenv(\"POSTGRES_USER\")}:{os.getenv(\"POSTGRES_PASSWORD\")}@{os.getenv(\"POSTGRES_HOST\")}:{os.getenv(\"POSTGRES_PORT\")}/{os.getenv(\"POSTGRES_DB\")}')\n",
                "    Base.metadata.create_all(engine)\n",
                "    \n",
                "    # Create a session\n",
                "    Session = sessionmaker(bind=engine)\n",
                "    session = Session()\n",
                "    \n",
                "    notebook_id = os.getenv('NOTEBOOK_ID')\n",
                "    \n",
                "    # Query for the specific entry\n",
                "    row_count = session.query(MyTable).filter(MyTable.notebook_id == notebook_id).delete(synchronize_session='evaluate')\n",
                "    \n",
                "    if row_count > 0:\n",
                "        session.commit()\n",
                "    else:\n",
                "        print(\"Rows not found\")\n",
                "    \n",
                "    session.close()\n",
                "    \n",
                "    response = requests.get(f\"http://{os.getenv('SERVICE_NAME')}:{os.getenv('SERVICE_PORT')}\")\n",
                "    if response.status_code == 200:\n",
                "    \n",
                "        response = requests.delete(f\"http://{os.getenv('SERVICE_NAME')}:{os.getenv('SERVICE_PORT')}/delete_pod?uid={notebook_id}\")\n",
                "    \n",
                "        if response.status_code == 200:\n",
                "            print(\"Pod deleted successfully!\")\n",
                "else:\n",
                "    print(\"Please run all the previouse cell before running this one!\")"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "authorship_tag": "ABX9TyPMHfVOTf5YAyvYaf+TB3Jq",
            "gpuType": "T4",
            "machine_shape": "hm",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
